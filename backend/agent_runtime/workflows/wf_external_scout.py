"""
WF-07: External Scout Pipeline

ì™¸ë¶€ ì„¸ë¯¸ë‚˜/ì´ë²¤íŠ¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ Activityë¡œ ë“±ë¡í•˜ëŠ” ì›Œí¬í”Œë¡œ

ì§€ì› ì†ŒìŠ¤:
- RSS í”¼ë“œ (ê¸°ìˆ  ë¸”ë¡œê·¸, ì´ë²¤íŠ¸ ì‚¬ì´íŠ¸)
- OnOffMix (ì˜¨ì˜¤í”„ë¯¹ìŠ¤) - í•œêµ­ IT/ìŠ¤íƒ€íŠ¸ì—… ì´ë²¤íŠ¸
- EventUs (ì´ë²¤í„°ìŠ¤) - í•œêµ­ IT ì´ë²¤íŠ¸
- DevEvent (GitHub) - ê°œë°œì ì»¤ë®¤ë‹ˆí‹° ì´ë²¤íŠ¸
- Eventbrite - ê¸€ë¡œë²Œ ì´ë²¤íŠ¸

âš ï¸ DEPRECATED ì†ŒìŠ¤:
- Festa (festa.io) - 2025.01.31 ì„œë¹„ìŠ¤ ì¢…ë£Œ

íë¦„:
1. ê° ì†ŒìŠ¤ì—ì„œ AI/AX ê´€ë ¨ ì„¸ë¯¸ë‚˜ ì •ë³´ ìˆ˜ì§‘
2. AI/AX í‚¤ì›Œë“œ í•„í„°ë§
3. ì¤‘ë³µ ì œê±° (URL, external_id ê¸°ì¤€)
4. Activity ìƒì„± ë° DB ì €ì¥
5. Confluence Action Log ê¸°ë¡
6. ê²°ê³¼ ìš”ì•½ ë°˜í™˜
"""

import os
from dataclasses import dataclass, field
from datetime import UTC, datetime
from typing import Any

import structlog
from sqlalchemy.ext.asyncio import AsyncSession

from backend.database.repositories.activity import activity_repo
from backend.integrations.external_sources import (
    AI_AX_KEYWORDS,
    DevEventCollector,
    EventbriteCollector,
    EventUsCollector,
    FestaCollector,
    OnOffMixCollector,
    RSSCollector,
    SeminarInfo,
)

logger = structlog.get_logger()


@dataclass
class ExternalScoutInput:
    """ì™¸ë¶€ ìŠ¤ì¹´ìš°íŠ¸ ì…ë ¥ ë°ì´í„°"""

    # ìˆ˜ì§‘ ì†ŒìŠ¤ (rss, onoffmix, eventus, devevent, eventbrite)
    # âš ï¸ festaëŠ” 2025.01.31 ì„œë¹„ìŠ¤ ì¢…ë£Œë¡œ ì‚¬ìš© ë¶ˆê°€
    sources: list[str] = field(
        default_factory=lambda: ["rss", "onoffmix", "eventus", "devevent", "eventbrite"]
    )

    # í•„í„°ë§ (ê¸°ë³¸: AI/AX í‚¤ì›Œë“œ)
    keywords: list[str] | None = field(
        default_factory=lambda: AI_AX_KEYWORDS[:10]
    )  # ìƒìœ„ 10ê°œ AI/AX í‚¤ì›Œë“œ
    categories: list[str] | None = None  # ì¹´í…Œê³ ë¦¬ í•„í„°

    # ì†ŒìŠ¤ë³„ ì„¤ì •
    rss_feed_urls: list[str] | None = None  # RSS í”¼ë“œ URL ëª©ë¡
    onoffmix_categories: list[str] | None = None  # ì˜¨ì˜¤í”„ë¯¹ìŠ¤ ì¹´í…Œê³ ë¦¬ (it, startup)
    eventus_categories: list[str] | None = None  # ì´ë²¤í„°ìŠ¤ ì¹´í…Œê³ ë¦¬ (it, startup)
    devevent_months_back: int = 2  # Dev-Event ëª‡ ê°œì›” ì „ê¹Œì§€ ìˆ˜ì§‘
    eventbrite_location: str | None = None  # Eventbrite ì§€ì—­
    eventbrite_organizer_ids: list[str] | None = None  # Eventbrite ì£¼ìµœì ID

    # ê³µí†µ ì„¤ì •
    limit_per_source: int = 50  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
    play_id: str = "EXT_Desk_D01_Seminar"  # ê¸°ë³¸ Play ID
    save_to_db: bool = True  # DB ì €ì¥ ì—¬ë¶€
    sync_confluence: bool = True  # Confluence ë™ê¸°í™” ì—¬ë¶€


@dataclass
class ExternalScoutOutput:
    """ì™¸ë¶€ ìŠ¤ì¹´ìš°íŠ¸ ì¶œë ¥ ë°ì´í„°"""

    # ìˆ˜ì§‘ ê²°ê³¼
    total_collected: int  # ì´ ìˆ˜ì§‘ëœ ì„¸ë¯¸ë‚˜ ìˆ˜
    total_saved: int  # DBì— ì €ì¥ëœ Activity ìˆ˜
    duplicates_skipped: int  # ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µëœ ìˆ˜
    errors: list[dict[str, Any]]  # ì—ëŸ¬ ëª©ë¡

    # ì†ŒìŠ¤ë³„ í†µê³„
    by_source: dict[str, dict[str, int]]  # {"rss": {"collected": 10, "saved": 8}}

    # ì €ì¥ëœ Activity ëª©ë¡
    activities: list[dict[str, Any]]

    # Confluence ë™ê¸°í™” ê²°ê³¼
    confluence_updated: bool
    confluence_url: str | None

    # ë©”íƒ€ë°ì´í„°
    started_at: str
    finished_at: str
    duration_seconds: float


class ExternalScoutPipeline:
    """
    WF-07: External Scout Pipeline

    ì™¸ë¶€ ì„¸ë¯¸ë‚˜ ì •ë³´ ë°°ì¹˜ ìˆ˜ì§‘
    """

    def __init__(self):
        self.logger = logger.bind(workflow="WF-07")

        # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        self.collectors = {
            "rss": RSSCollector(),
            "onoffmix": OnOffMixCollector(),
            "eventus": EventUsCollector(),
            "devevent": DevEventCollector(),
            "eventbrite": EventbriteCollector(),
            # DEPRECATED: 2025.01.31 ì„œë¹„ìŠ¤ ì¢…ë£Œ
            "festa": FestaCollector(),
        }

    async def run(self, input_data: ExternalScoutInput) -> ExternalScoutOutput:
        """ì›Œí¬í”Œë¡œ ì‹¤í–‰"""
        started_at = datetime.now(UTC)
        self.logger.info(
            "Starting external scout pipeline",
            sources=input_data.sources,
            keywords=input_data.keywords,
        )

        # ê²°ê³¼ ì´ˆê¸°í™”
        all_seminars: list[SeminarInfo] = []
        by_source: dict[str, dict[str, int]] = {}
        errors: list[dict[str, Any]] = []

        # 1. ê° ì†ŒìŠ¤ì—ì„œ ì„¸ë¯¸ë‚˜ ìˆ˜ì§‘
        for source in input_data.sources:
            if source not in self.collectors:
                self.logger.warning(f"Unknown source: {source}")
                continue

            try:
                seminars = await self._collect_from_source(source, input_data)
                all_seminars.extend(seminars)
                by_source[source] = {
                    "collected": len(seminars),
                    "saved": 0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                }
                self.logger.info(
                    "Source collection completed",
                    source=source,
                    count=len(seminars),
                )
            except Exception as e:
                self.logger.error(
                    "Source collection failed",
                    source=source,
                    error=str(e),
                )
                errors.append(
                    {
                        "source": source,
                        "error": str(e),
                        "type": "collection",
                    }
                )

        total_collected = len(all_seminars)
        self.logger.info(f"Total collected: {total_collected} seminars")

        # 2. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_seminars = []
        for seminar in all_seminars:
            if seminar.url not in seen_urls:
                seen_urls.add(seminar.url)
                unique_seminars.append(seminar)

        duplicates_in_batch = total_collected - len(unique_seminars)
        self.logger.info(
            "Duplicates removed in batch",
            original=total_collected,
            unique=len(unique_seminars),
            removed=duplicates_in_batch,
        )

        # 3. Activity ìƒì„± ê²°ê³¼
        activities: list[dict[str, Any]] = []
        total_saved = 0
        duplicates_skipped = duplicates_in_batch

        # 4. DB ì €ì¥ (ì˜µì…˜)
        if not input_data.save_to_db:
            # DB ì €ì¥ ì—†ì´ ìˆ˜ì§‘ ê²°ê³¼ë§Œ ë°˜í™˜
            for seminar in unique_seminars:
                activity_data = seminar.to_activity_data()
                activity_data["play_id"] = input_data.play_id
                activities.append(activity_data)
        else:
            self.logger.info("DB ì €ì¥ì€ ExternalScoutPipelineWithDBë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
            for seminar in unique_seminars:
                activity_data = seminar.to_activity_data()
                activity_data["play_id"] = input_data.play_id
                activities.append(activity_data)

        # 5. ê²°ê³¼ ìƒì„±
        finished_at = datetime.now(UTC)
        duration = (finished_at - started_at).total_seconds()

        result = ExternalScoutOutput(
            total_collected=total_collected,
            total_saved=total_saved,
            duplicates_skipped=duplicates_skipped,
            errors=errors,
            by_source=by_source,
            activities=activities,
            confluence_updated=False,
            confluence_url=None,
            started_at=started_at.isoformat(),
            finished_at=finished_at.isoformat(),
            duration_seconds=duration,
        )

        self.logger.info(
            "External scout pipeline completed",
            total_collected=total_collected,
            unique=len(unique_seminars),
            duration=duration,
        )

        return result

    async def _collect_from_source(
        self,
        source: str,
        input_data: ExternalScoutInput,
    ) -> list[SeminarInfo]:
        """
        ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ ì„¸ë¯¸ë‚˜ ìˆ˜ì§‘

        Args:
            source: ì†ŒìŠ¤ íƒ€ì…
            input_data: ì…ë ¥ ë°ì´í„°

        Returns:
            list[SeminarInfo]: ìˆ˜ì§‘ëœ ì„¸ë¯¸ë‚˜ ëª©ë¡
        """
        collector = self.collectors[source]

        # ì†ŒìŠ¤ë³„ íŒŒë¼ë¯¸í„° êµ¬ì„±
        kwargs: dict[str, Any] = {}

        if source == "rss":
            # RSS í”¼ë“œ URL
            feed_urls = input_data.rss_feed_urls or self._get_default_rss_feeds()
            kwargs["feed_urls"] = feed_urls

        elif source == "onoffmix":
            # ì˜¨ì˜¤í”„ë¯¹ìŠ¤ ì¹´í…Œê³ ë¦¬
            if input_data.onoffmix_categories:
                kwargs["categories"] = input_data.onoffmix_categories

        elif source == "eventus":
            # ì´ë²¤í„°ìŠ¤ ì¹´í…Œê³ ë¦¬
            if input_data.eventus_categories:
                kwargs["categories"] = input_data.eventus_categories

        elif source == "devevent":
            # Dev-Event ìˆ˜ì§‘ ê¸°ê°„
            kwargs["months_back"] = input_data.devevent_months_back

        elif source == "eventbrite":
            # Eventbrite ì§€ì—­ ë° ì£¼ìµœì
            if input_data.eventbrite_location:
                kwargs["location"] = input_data.eventbrite_location
            if input_data.eventbrite_organizer_ids:
                kwargs["organizer_ids"] = input_data.eventbrite_organizer_ids

        elif source == "festa":
            # DEPRECATED: ê²½ê³  ë¡œê·¸ë§Œ ì¶œë ¥, ë¹ˆ ê²°ê³¼ ë°˜í™˜
            self.logger.warning(
                "Festa ìˆ˜ì§‘ê¸°ëŠ” ì„œë¹„ìŠ¤ ì¢…ë£Œë¡œ ì‚¬ìš© ë¶ˆê°€",
                source=source,
                service_ended="2025-01-31",
            )

        # ê³µí†µ íŒŒë¼ë¯¸í„°
        seminars = await collector.fetch_seminars(
            keywords=input_data.keywords,
            categories=input_data.categories,
            limit=input_data.limit_per_source,
            **kwargs,
        )

        return seminars

    def _get_default_rss_feeds(self) -> list[str]:
        """ê¸°ë³¸ RSS í”¼ë“œ URL ëª©ë¡"""
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        feeds_env = os.getenv("SEMINAR_RSS_FEEDS", "")
        if feeds_env:
            return [f.strip() for f in feeds_env.split(",") if f.strip()]

        # ê¸°ë³¸ í”¼ë“œ (ì˜ˆì‹œ)
        return [
            # ê¸°ìˆ  ì»¨í¼ëŸ°ìŠ¤/ì„¸ë¯¸ë‚˜ í”¼ë“œ ì˜ˆì‹œ
            # "https://example.com/tech-events.rss",
        ]


class ExternalScoutPipelineWithDB(ExternalScoutPipeline):
    """
    WF-07: External Scout Pipeline with DB

    DB ì €ì¥ ë° Confluence ë™ê¸°í™” í¬í•¨
    """

    def __init__(self, db: AsyncSession):
        super().__init__()
        self.db = db
        self.logger = logger.bind(workflow="WF-07", with_db=True)

    async def run(self, input_data: ExternalScoutInput) -> ExternalScoutOutput:
        """ì›Œí¬í”Œë¡œ ì‹¤í–‰ (DB ì €ì¥ í¬í•¨)"""
        started_at = datetime.now(UTC)
        self.logger.info(
            "Starting external scout pipeline with DB",
            sources=input_data.sources,
            keywords=input_data.keywords,
        )

        # ê²°ê³¼ ì´ˆê¸°í™”
        all_seminars: list[SeminarInfo] = []
        by_source: dict[str, dict[str, int]] = {}
        errors: list[dict[str, Any]] = []

        # 1. ê° ì†ŒìŠ¤ì—ì„œ ì„¸ë¯¸ë‚˜ ìˆ˜ì§‘
        for source in input_data.sources:
            if source not in self.collectors:
                self.logger.warning(f"Unknown source: {source}")
                continue

            try:
                seminars = await self._collect_from_source(source, input_data)
                all_seminars.extend(seminars)
                by_source[source] = {
                    "collected": len(seminars),
                    "saved": 0,
                }
            except Exception as e:
                self.logger.error(
                    "Source collection failed",
                    source=source,
                    error=str(e),
                )
                errors.append(
                    {
                        "source": source,
                        "error": str(e),
                        "type": "collection",
                    }
                )

        total_collected = len(all_seminars)

        # 2. ì¤‘ë³µ ì œê±° ë° DB ì €ì¥
        activities: list[dict[str, Any]] = []
        total_saved = 0
        duplicates_skipped = 0

        for seminar in all_seminars:
            try:
                # ì¤‘ë³µ ì²´í¬
                existing = await activity_repo.check_duplicate(
                    self.db,
                    url=seminar.url,
                    title=seminar.title,
                    date=seminar.date,
                    external_id=seminar.external_id,
                )

                if existing:
                    duplicates_skipped += 1
                    self.logger.debug(
                        "Skipping duplicate activity",
                        url=seminar.url,
                        existing_id=existing.entity_id,
                    )
                    continue

                # Activity ìƒì„±
                activity_data = seminar.to_activity_data()
                activity_data["play_id"] = input_data.play_id

                entity = await activity_repo.create_activity(self.db, activity_data)
                total_saved += 1

                # ì†ŒìŠ¤ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                source_type = seminar.source_type
                if source_type in by_source:
                    by_source[source_type]["saved"] += 1

                activities.append(entity.to_dict())

                self.logger.info(
                    "Activity saved",
                    activity_id=entity.entity_id,
                    title=entity.name[:50],
                )

            except Exception as e:
                self.logger.error(
                    "Failed to save activity",
                    url=seminar.url,
                    error=str(e),
                )
                errors.append(
                    {
                        "url": seminar.url,
                        "error": str(e),
                        "type": "save",
                    }
                )

        # ì»¤ë°‹
        await self.db.commit()

        # 3. Confluence ë™ê¸°í™” (ì˜µì…˜)
        confluence_updated = False
        confluence_url = None

        if input_data.sync_confluence and total_saved > 0:
            try:
                confluence_result = await self._sync_to_confluence(
                    activities,
                    input_data.play_id,
                )
                confluence_updated = confluence_result.get("success", False)
                confluence_url = confluence_result.get("page_url")
            except Exception as e:
                self.logger.error("Confluence sync failed", error=str(e))
                errors.append(
                    {
                        "type": "confluence_sync",
                        "error": str(e),
                    }
                )

        # 4. ê²°ê³¼ ìƒì„±
        finished_at = datetime.now(UTC)
        duration = (finished_at - started_at).total_seconds()

        result = ExternalScoutOutput(
            total_collected=total_collected,
            total_saved=total_saved,
            duplicates_skipped=duplicates_skipped,
            errors=errors,
            by_source=by_source,
            activities=activities,
            confluence_updated=confluence_updated,
            confluence_url=confluence_url,
            started_at=started_at.isoformat(),
            finished_at=finished_at.isoformat(),
            duration_seconds=duration,
        )

        self.logger.info(
            "External scout pipeline with DB completed",
            total_collected=total_collected,
            total_saved=total_saved,
            duplicates_skipped=duplicates_skipped,
            duration=duration,
        )

        return result

    async def _sync_to_confluence(
        self,
        activities: list[dict[str, Any]],
        play_id: str,
    ) -> dict[str, Any]:
        """
        Confluenceì— ìˆ˜ì§‘ ê²°ê³¼ ë™ê¸°í™”

        Args:
            activities: ì €ì¥ëœ Activity ëª©ë¡
            play_id: Play ID

        Returns:
            dict: ë™ê¸°í™” ê²°ê³¼
        """
        from backend.integrations.mcp_confluence.server import ConfluenceMCP

        mcp = ConfluenceMCP()

        try:
            # Action Logì— ë°°ì¹˜ ìˆ˜ì§‘ ê¸°ë¡ ì¶”ê°€
            action_log_page_id = os.getenv("CONFLUENCE_ACTION_LOG_PAGE_ID", "")
            if not action_log_page_id:
                return {"success": False, "error": "Action Log page ID not configured"}

            # ë¡œê·¸ í•­ëª© ìƒì„±
            now = datetime.now(UTC).strftime("%Y-%m-%d %H:%M KST")
            log_entry = f"""
## ğŸ¤– External Scout ë°°ì¹˜ ìˆ˜ì§‘ ({now})

**ìˆ˜ì§‘ ê²°ê³¼**:
- ì´ ìˆ˜ì§‘: {len(activities)}ê±´
- Play: {play_id}

| Activity ID | ì œëª© | ë‚ ì§œ | ì†ŒìŠ¤ |
|-------------|------|------|------|
"""
            for act in activities[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                log_entry += f"| {act.get('entity_id', '-')} | {act.get('name', '-')[:30]} | {act.get('properties', {}).get('date', '-')} | {act.get('properties', {}).get('source_type', '-')} |\n"

            if len(activities) > 10:
                log_entry += f"\n... ì™¸ {len(activities) - 10}ê±´\n"

            log_entry += "\n---\n"

            await mcp.append_to_page(page_id=action_log_page_id, append_md=log_entry)

            # Play DB ì—…ë°ì´íŠ¸ (activity_qtd ì¦ê°€)
            play_db_page_id = os.getenv("CONFLUENCE_PLAY_DB_PAGE_ID", "")
            if play_db_page_id:
                for _ in activities:
                    await mcp.increment_play_activity_count(
                        page_id=play_db_page_id,
                        play_id=play_id,
                    )

            return {
                "success": True,
                "page_url": f"https://your-confluence.atlassian.net/wiki/spaces/AX/pages/{action_log_page_id}",
            }

        except Exception as e:
            self.logger.error("Confluence sync error", error=str(e))
            return {"success": False, "error": str(e)}


# ì›Œí¬í”Œë¡œ ì¸ìŠ¤í„´ìŠ¤ (ê¸°ë³¸)
external_scout_pipeline = ExternalScoutPipeline()


async def run_external_scout(
    sources: list[str] | None = None,
    keywords: list[str] | None = None,
    categories: list[str] | None = None,
    limit_per_source: int = 50,
    play_id: str = "EXT_Desk_D01_Seminar",
) -> ExternalScoutOutput:
    """
    ì™¸ë¶€ ìŠ¤ì¹´ìš°íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í¸ì˜ í•¨ìˆ˜)

    AI/AX ê´€ë ¨ ì„¸ë¯¸ë‚˜ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    Args:
        sources: ìˆ˜ì§‘ ì†ŒìŠ¤ ëª©ë¡ (ê¸°ë³¸: rss, onoffmix, eventus, devevent, eventbrite)
        keywords: í•„í„° í‚¤ì›Œë“œ (ê¸°ë³¸: AI/AX ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ)
        categories: í•„í„° ì¹´í…Œê³ ë¦¬
        limit_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        play_id: Play ID

    Returns:
        ExternalScoutOutput: ìˆ˜ì§‘ ê²°ê³¼
    """
    input_data = ExternalScoutInput(
        sources=sources or ["rss", "onoffmix", "eventus", "devevent", "eventbrite"],
        keywords=keywords or AI_AX_KEYWORDS[:10],
        categories=categories,
        limit_per_source=limit_per_source,
        play_id=play_id,
        save_to_db=False,  # ê¸°ë³¸ ì‹¤í–‰ì€ DB ì €ì¥ ì•ˆí•¨
    )
    return await external_scout_pipeline.run(input_data)
